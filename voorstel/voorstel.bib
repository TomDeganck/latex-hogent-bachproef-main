@inbook{Cooper2011,
	title        = {Sign Language Recognition},
	author       = {Cooper, Helen and Holt, Brian and Bowden, Richard},
	year         = 2011,
	booktitle    = {Visual Analysis of Humans: Looking at People},
	publisher    = {Springer London},
	address      = {London},
	pages        = {539--562},
	doi          = {10.1007/978-0-85729-997-0_27},
	isbn         = {978-0-85729-997-0},
	url          = {https://doi.org/10.1007/978-0-85729-997-0_27},
	editor       = {Moeslund, Thomas B., Hilton, Adrian, Kr{\"u}ger, Volker, and Sigal, Leonid},
	abstract     = {This chapter covers the key aspects of sign-language recognition (SLR), starting with a brief introduction to the motivations and requirements, followed by a pr{\'e}cis of sign linguistics and their impact on the field. The types of data available and the relative merits are explored allowing examination of the features which can be extracted. Classifying the manual aspects of sign (similar to gestures) is then discussed from a tracking and non-tracking viewpoint before summarising some of the approaches to the non-manual aspects of sign languages. Methods for combining the sign classification results into full SLR are given showing the progression towards speech recognition techniques and the further adaptations required for the sign specific case. Finally the current frontiers are discussed and the recent research presented. This covers the task of continuous sign recognition, the work towards true signer independence, how to effectively combine the different modalities of sign, making use of the current linguistic research and adapting to larger more noisy data sets.},
	timestamp    = {2024-10-29}
}
@article{10.56726/irjmets32712,
	title        = {Real time sign language recognition using computer vision},
	year         = 2023,
	journal      = {International Research Journal of Modernization in Engineering Technology and Science},
	doi          = {10.56726/irjmets32712},
	timestamp    = {2024-11-04}
}
@article{10.1093/ijl/ecy008,
	title        = {Looking back while moving forward: the impact of societal and technological developments on flemish sign language lexicographic practices},
	author       = {Vermeerbergen, M. and Herreweghe, M.},
	year         = 2018,
	journal      = {International Journal of Lexicography},
	volume       = 31,
	pages        = {167--195},
	doi          = {10.1093/ijl/ecy008},
	issue        = 2,
	timestamp    = {2024-11-04}
}
@article{10.1109/iccv.2019.00550,
	title        = {Fingerspelling recognition in the wild with iterative visual attention},
	author       = {Shi, B. and Rio, A. and Keane, J. and Brentari, D. and Shakhnarovich, G. and Livescu, K.},
	year         = 2019,
	doi          = {10.1109/iccv.2019.00550},
	timestamp    = {2024-11-04}
}
@article{10.11591/csit.v3i3.p186-193,
	title        = {Pakistan sign language to urdu translator using kinect},
	author       = {Ahmed, S. and Shafiq, H. and Raheel, Y. and Chishti, N. and Asad, S.},
	year         = 2022,
	journal      = {Computer Science and Information Technologies},
	volume       = 3,
	pages        = {186--193},
	doi          = {10.11591/csit.v3i3.p186-193},
	issue        = 3,
	timestamp    = {2024-11-04}
}
@article{10.48550/arxiv.2012.15840,
	title        = {Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers},
	author       = {Zheng, S. and Lu, J. and Zhao, H. and Zhu, X. and Luo, Z. and Wang, Y. and Fu, Y. and Feng, J. and Xiang, T. and Torr, P. and Zhang, L.},
	year         = 2020,
	doi          = {10.48550/arxiv.2012.15840},
	timestamp    = {2024-11-04}
}
@article{10.57197/jdr-2023-0047,
	title        = {Sign language recognition using artificial rabbits optimizer with siamese neural network for persons with disabilities},
	author       = {Marzouk, R.},
	year         = 2023,
	volume       = 2,
	doi          = {10.57197/jdr-2023-0047},
	issue        = 4,
	timestamp    = {2024-11-04}
}
@article{10.22214/ijraset.2023.52628,
	title        = {Ai-based sign language recognition system for deaf and hard-of-hearing individuals},
	author       = {Gade, P.},
	year         = 2023,
	journal      = {International Journal for Research in Applied Science and Engineering Technology},
	volume       = 11,
	pages        = {4472--4476},
	doi          = {10.22214/ijraset.2023.52628},
	issue        = 5,
	timestamp    = {2024-11-04}
}
@article{10.1109/wacv45572.2020.9093512,
	title        = {Word-level deep sign language recognition from video: a new large-scale dataset and methods comparison},
	author       = {Li, D. and Opazo, C. and Yu, X. and Li, H.},
	year         = 2020,
	pages        = {1448--1458},
	doi          = {10.1109/wacv45572.2020.9093512},
	timestamp    = {2024-11-04}
}
@article{10.18178/ijmlc.2019.9.6.879,
	title        = {Static sign language recognition using deep learning},
	author       = {Tolentino, L. and Juan, R. and Thio-ac, A. and Pamahoy, M. and Forteza, J. and Garcia, X.},
	year         = 2019,
	journal      = {International Journal of Machine Learning and Computing},
	volume       = 9,
	pages        = {821--827},
	doi          = {10.18178/ijmlc.2019.9.6.879},
	issue        = 6,
	timestamp    = {2024-11-04}
}
@article{10.24003/emitter.v9i1.613,
	title        = {Indian sign language recognition through hybrid convnet-lstm networks},
	author       = {H, M.},
	year         = 2021,
	journal      = {Emitter International Journal of Engineering Technology},
	volume       = 9,
	pages        = {182--203},
	doi          = {10.24003/emitter.v9i1.613},
	issue        = 1,
	timestamp    = {2024-11-04}
}
@article{10.18653/v1/2020.conll-1.51,
	title        = {A dataset for linguistic understanding, visual evaluation, and recognition of sign languages: the k-rsl},
	author       = {Imashev, A. and Mukushev, M. and Kimmelman, V. and Sandygulova, A.},
	year         = 2020,
	doi          = {10.18653/v1/2020.conll-1.51},
	timestamp    = {2024-11-04}
}
@article{10.1049/iet-cvi.2017.0598,
	title        = {Multi-modality-based arabic sign language recognition},
	author       = {Elpeltagy, M. and Abdelwahab, M. and Hussein, M. and Shoukry, A. and Shoala, A. and Galal, M.},
	year         = 2018,
	journal      = {Iet Computer Vision},
	volume       = 12,
	pages        = {1031--1039},
	doi          = {10.1049/iet-cvi.2017.0598},
	issue        = 7,
	timestamp    = {2024-11-04}
}
@article{10.1109/ictas.2019.8703631,
	title        = {Video relay service for deaf people using webrtc},
	author       = {Henney, A. and Tucker, W.},
	year         = 2019,
	pages        = {1--6},
	doi          = {10.1109/ictas.2019.8703631},
	timestamp    = {2024-11-04}
}
@article{10.22214/ijraset.2022.46470,
	title        = {A survey on real time sign language detector using ml},
	author       = {Gattani, P. and Laddha, S. and Srivastava, S. and BM, V.},
	year         = 2022,
	journal      = {International Journal for Research in Applied Science and Engineering Technology},
	volume       = 10,
	pages        = {1635--1643},
	doi          = {10.22214/ijraset.2022.46470},
	issue        = 8,
	timestamp    = {2024-11-04}
}
@INPROCEEDINGS{10.1109/compsac51774.2021.00173
,
  author={Buckley, Neil and Sherrett, Lewis and Lindo Secco, Emanuele},
  booktitle={2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)}, 
  title={A CNN sign language recognition system with single & double-handed gestures}, 
  year={2021},
  volume={},
  number={},
  pages={1250-1253},
  keywords={Computer vision;Conferences;Bibliographies;Gesture recognition;Computer architecture;Assistive technologies;Software;sign language recognition;AI;CNN;human-machine interaction},
  doi={10.1109/COMPSAC51774.2021.00173}}

@article{10.1109/access.2020.2990699,
	title        = {Deeparslr: a novel signer-independent deep learning framework for isolated arabic sign language gestures recognition},
	author       = {Aly, S. and Aly, W.},
	year         = 2020,
	journal      = {Ieee Access},
	volume       = 8,
	pages        = {83199--83212},
	doi          = {10.1109/access.2020.2990699},
	timestamp    = {2024-11-04}
}
@article{10.1109/access.2020.2993650,
	title        = {Continuous sign language recognition through cross-modal alignment of video and text embeddings in a joint-latent space},
	author       = {Papastratis, I. and Dimitropoulos, K. and Konstantinidis, D. and Daras, P.},
	year         = 2020,
	journal      = {Ieee Access},
	volume       = 8,
	pages        = {91170--91180},
	doi          = {10.1109/access.2020.2993650},
	timestamp    = {2024-11-04}
}
@article{10.13053/rcs-148-3-17,
	title        = {Data acquisition software for sign language recognition},
	author       = {Mart\'{\i}nez-Guti\'{e}rrez, M. and Rojano-C\'{a}ceres, J. and Ben\'{\i}tez–Guerrero, E. and S\'{a}nchez-Barrera, H.},
	year         = 2019,
	journal      = {Research in Computing Science},
	volume       = 148,
	pages        = {205--211},
	doi          = {10.13053/rcs-148-3-17},
	issue        = 3,
	timestamp    = {2024-11-04}
}
@article{10.1002/cpe.7653,
	title        = {A review on computational methods based automated sign language recognition system for hearing and speech impaired community},
	author       = {Robert, E. and Hemanth, D.},
	year         = 2023,
	journal      = {Concurrency and Computation Practice and Experience},
	volume       = 35,
	doi          = {10.1002/cpe.7653},
	issue        = 9,
	timestamp    = {2024-11-04}
}
@article{10.13053/rcs-148-6-19,
	title        = {Sign language recognition based on emg signals through a hibrid intelligent system},
	author       = {Rodr\'{\i}guez-Tapia, B. and Ochoa-Zezzatti, A. and Marrufo, \'{A}. and Arballo, N. and Carlos, P.},
	year         = 2019,
	journal      = {Research in Computing Science},
	volume       = 148,
	pages        = {253--262},
	doi          = {10.13053/rcs-148-6-19},
	issue        = 6,
	timestamp    = {2024-11-04}
}
@article{10.1007/978-3-030-66096-3_13,
	title        = {Slrtp 2020: the sign language recognition, translation \&amp; production workshop},
	author       = {Camg\"{o}z, N. and Varol, G. and Albanie, S. and Fox, N. and Bowden, R. and Zisserman, A. and Cormier, K.},
	year         = 2020,
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {179--185},
	doi          = {10.1007/978-3-030-66096-3_13},
	timestamp    = {2024-11-04}
}
@article{10.32604/cmc.2022.025953,
	title        = {Intelligent sign language recognition system for e-learning context},
	author       = {Hussain, M. and Shaoor, A. and Alsuhibany, S. and Ghadi, Y. and Shloul, T. and Jalal, A. and Park, J.},
	year         = 2022,
	journal      = {Computers Materials \& Continua},
	volume       = 72,
	pages        = {5327--5343},
	doi          = {10.32604/cmc.2022.025953},
	issue        = 3,
	timestamp    = {2024-11-04}
}
@article{10.48550/arxiv.2010.11929,
	title        = {An image is worth 16x16 words: transformers for image recognition at scale},
	author       = {Dosovitskiy, A.},
	year         = 2020,
	doi          = {10.48550/arxiv.2010.11929},
	timestamp    = {2024-11-04}
}
@article{10.1145/3308561.3353774,
	title        = {Sign language recognition, generation, and translation},
	author       = {Bragg, D. and Koller, O. and Bellard, M. and Berke, L. and Boudreault, P. and Braffort, A. and Caselli, N. and Huenerfauth, M. and Kacorri, H. and Verhoef, T. and Vogler, C. and Morris, M.},
	year         = 2019,
	pages        = {16--31},
	doi          = {10.1145/3308561.3353774},
	timestamp    = {2024-11-04}
}
@article{vanmeerbergen2000simultane,
	title        = {Simultane constructies in de Vlaamse Gebarentaal},
	author       = {Vanmeerbergen, Myriam},
	year         = 2000,
	journal      = {Handelingen-Koninklijke Zuid-Nederlandse maatschappij voor taal-en letterkunde en geschiedenis},
	publisher    = {Koninklijke Zuid-Nederlandse Maatschappij voor Taal-en Letterkunde en~\ldots{}},
	volume       = 54,
	url          = {https://openjournals.ugent.be/kzm/article/id/72219/download/pdf/},
	timestamp    = {2024-11-05}
}
@phdthesis{469340,
	title        = {Fonologie van de Vlaamse Gebarentaal: distinctiviteit en iconiciteit},
	author       = {Demey, Eline},
	year         = 2005,
	publisher    = {Universiteit Gent. Faculteit Letteren en Wijsbegeerte},
	pages        = 550,
	url          = {http://lib.ugent.be/fulltxt/RUG01/000/938/845/RUG01-000938845_2010_0001_AC.pdf},
	abstract     = {De doelstelling van dit proefschrift is een eerste uitgebreide beschrijving te geven van de fonologische vormelementen en structuren in de Vlaamse Gebarentaal (VGT) en na te gaan wat daarbij de plaats van iconiciteit binnen (of buiten?) een fonologisch model kan zijn. De focus ligt op het manuele gedeelte van gebaren, d.w.z. de vier parameters handvorm, ori\"{e}ntatie van de hand, articulatieplaats en beweging van het gebaar. De basis voor de fonologische beschrijving is een gedetailleerde fonetische transcriptie van een 2400-tal ge\"{\i}soleerde VGT-gebaren (gebaren in citeervorm), getranscribeerd met een vereenvoudigde versie van SignPhon (Crasborn et al. 1997). De fonologische analyse van die gebaren gebeurt aan de hand van een model dat ontwikkeld is voor de Nederlandse Gebarentaal (NGT), met name het dependentiemodel van Van der Kooij (2002).},
	keywords     = {transcriptie,fonologie,iconiciteit,dependentiemodel,Vlaamse Gebarentaal (VGT)},
	language     = {dut},
	school       = {Ghent University},
	timestamp    = {2024-11-05}
}
@article{10.52756/ijerr.2023.v34spl.004,
	title        = {Machine learning-based gesture recognition for communication with the deaf and dumb},
	author       = {Yavanamandha, P. and Keerthana, B. and Jahnavi, P. and Rao, K. V. and Kumar, C. R.},
	year         = 2023,
	journal      = {International Journal of Experimental Research and Review},
	volume       = 34,
	pages        = {26--35},
	doi          = {10.52756/ijerr.2023.v34spl.004},
	issue        = {Special Vo},
	timestamp    = {2024-11-15}
}
@article{10.17485/ijst/v16i45.2583,
	title        = {Transforming sign language into text and speech through deep learning technologies},
	author       = {Duraisamy, P. and Abinayasrijanani, A. and Candida, M. A. and Babu, P. D.},
	year         = 2023,
	journal      = {Indian Journal of Science and Technology},
	volume       = 16,
	pages        = {4177--4185},
	doi          = {10.17485/ijst/v16i45.2583},
	issue        = 45,
	timestamp    = {2024-11-15}
}
@inproceedings{9491631,
	title        = {Deep CNN Model based on VGG16 for Breast Cancer Classification},
	author       = {Albashish, Dheeb and Al-Sayyed, Rizik and Abdullah, Azizi and Ryalat, Mohammad Hashem and Ahmad Almansour, Nedaa},
	year         = 2021,
	booktitle    = {2021 International Conference on Information Technology (ICIT)},
	pages        = {805--810},
	doi          = {10.1109/ICIT52682.2021.9491631},
	keywords     = {Deep learning;Adaptation models;Visualization;Histopathology;Computational modeling;Benchmark testing;Feature extraction;Deep learning;VGG16;BreaKHis histopathology dataset;feature extraction;pre-trained model},
	timestamp    = {2024-11-15}
}
@incollection{torrey2010transfer,
author = {Olivas, Emilio Soria and Guerrero, Jose David Martin and Sober, Marcelino Martinez and Benedito, Jose Rafael Magdalena and Lopez, Antonio Jose Serrano},
title = {Handbook Of Research On Machine Learning Applications and Trends: Algorithms, Methods and Techniques - 2 Volumes},
year = {2009},
isbn = {1605667668},
publisher = {Information Science Reference - Imprint of: IGI Publishing},
address = {Hershey, PA},
abstract = {The machine learning approach provides a useful tool when the amount of data is very large and a model is not available to explain the generation and relation of the data set. The Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques provides a set of practical applications for solving problems and applying various techniques in automatic data extraction and setting. A defining collection of field advancements, this Handbook of Research fills the gap between theory and practice, providing a strong reference for academicians, researchers, and practitioners.}

}
@article{Abu-Jamie2022-ABUCOS-2,
	title        = {Classification of Sign-Language Using Vgg16},
	author       = {Tanseem N. Abu-Jamie and Samy S. Abu-Naser},
	year         = 2022,
	journal      = {International Journal of Academic Engineering Research (IJAER)},
	volume       = 6,
	number       = 6,
	url         = {https://philpapers.org/archive/ABUCOS-2.pdf},
	pages        = {36--46},
	timestamp    = {2024-11-15},
}
@article{Wang2022,
	title        = {Improved 3D-ResNet sign language recognition algorithm with enhanced hand features},
	author       = {Shiqi Wang and Kankan Wang and Tingping Yang and Yiming Li and Di Fan},
	year         = 2022,
	journal      = {Scientific Reports},
	volume       = 12,
	number       = 1,
	pages        = 17812,
	doi          = {10.1038/s41598-022-21636-z},
	issn         = {2045-2322},
	url          = {https://doi.org/10.1038/s41598-022-21636-z},
	note         = {Published: 2022/10/24},
	abstract     = {In sign language video, the hand region is small, the resolution is low, the motion speed is fast, and there are cross occlusion and blur phenomena, which have a great impact on sign language recognition rate and speed, and are important factors restricting sign language recognition performance. To solve these problems, this paper proposes an improved 3D-ResNet sign language recognition algorithm with enhanced hand features, aiming to highlight the features of both hands, solve the problem of missing more effective information when relying only on global features, and improve the accuracy of sign language recognition. The proposed method has two improvements. Firstly, the algorithm detects the left and right hand regions based on the improved EfficientDet network, uses the improved Bi-FPN module and dual channel and spatial attention module are used to enhance the detection ability of the network for small targets like hand. Secondly, the improved residual module is used to improve the 3D-ResNet18 network to extract sign language features. The global, the left-hand and the right-hand image sequences are divided into three branches for feature extraction and fusion, so as to strengthen the attention to hand features, strengthen the representation ability of sign language features, and achieve the purpose of improving the accuracy of sign language recognition. In order to verify the performance of this algorithm, a series of experiments are carried out on CSL dataset. For example, in the experiments of hand detection algorithm and sign language recognition algorithm, the performance indicators such as Top-N, mAP, FLOPs and Parm are applied to find the optimal algorithm framework. The experimental results show that the Top1 recognition accuracy of this algorithm reaches 91.12\%, which is more than 10\% higher than that of C3D, P3D and 3D-ResNet basic networks. From the performance indicators of Top-N, mAP, FLOPs, Parm and so on, the performance of the algorithm in this paper is better than several algorithms in recent three years, such as I3D+BLSTM, B3D ResNet, AM-ResC3D+RCNN and so on. The results show that the hand detection network with enhanced hand features and three-dimensional convolutional neural network proposed in this paper can achieve higher accuracy of sign language recognition.},
	timestamp    = {2024-11-15}
}
@misc{Coster2023,
	title        = {Towards the extraction of robust sign embeddings for low resource sign language recognition},
	author       = {Coster, Mathieu and Rushe, Ellen and Holmes, Ruth and Ventresque, Anthony and Dambre, J.},
	year         = 2023,
	month        = {06},
	doi          = {10.48550/arXiv.2306.17558},
	timestamp    = {2024-11-15}
}
@misc{Corpus_VGT,
	title        = {Het Corpus VGT. Een digitaal open access corpus van videos and annotaties van Vlaamse Gebarentaal, ontwikkeld aan de Universiteit Gent ism KU Leuven. <www.corpusvgt.be>},
	author       = {Van Herreweghe, Mieke and Vermeerbergen, Myriam and Demey, Eline and De Durpel, Hannes and Nyffels, Hilde and Verstraete, Sam},
	year         = 2015,
	url          = {http://www.corpusvgt.ugent.be/},
	abstract     = {
		Het Corpus VGT is een verzameling van video's in Vlaamse Gebarentaal. We verzamelden in totaal meer dan 5 TB of 140 uren video. 120 dove mensen werkten mee aan het Corpus VGT als informanten. Bij het selecteren van de informanten werd rekening gehouden met leeftijd, regio en gender. Per twee gaven we hen een reeks thema's om over te praten: vertellen van een verhaal, maken van afspraken, discussi\"{e}ren over een thema, vertellen over hun schooltijd enz. Deze conversaties namen we op video op en we monteerden ze per opdracht. De gesprekken in Vlaamse Gebarentaal annoteerden we vervolgens door ze te glossen.

		Het Corpus VGT is gemaakt om onderzoek naar Vlaamse Gebarentaal te doen. Omdat er in andere landen ook al gebarentaalcorpora zijn samengesteld, kan men ook vergelijkend onderzoek tussen Vlaamse Gebarentaal en andere gebarentalen doen. Daarnaast kunnen docenten het corpus gebruiken in hun lessen Vlaamse Gebarentaal. Het Corpus VGT is ook een middel om Vlaamse Gebarentaal en aspecten van de Vlaamse Dovencultuur te documenteren en te bewaren.
	},
	keywords     = {Vlaamse Gebarentaal,Corpuslingu\"{\i}stiek},
	language     = {dut},
	timestamp    = {2024-11-15}
}
@misc{VGT_Signbank,
	url          = {https://signbank.vlaamsegebarentaal.be/accounts/login/?next=/datasets/available},
	timestamp    = {2024-11-15}
}
@misc{Over_het_Corpus_VGT,
	url          = {https://www.corpusvgt.be/},
	timestamp    = {2024-11-15}
}

@Inbook{Vandeghinste2024,
author="Vandeghinste, Vincent
and De Sisto, Mirella
and G{\'o}mez, Santiago Egea
and De Coster, Mathieu",
editor="Way, Andy
and Leeson, Lorraine
and Shterionov, Dimitar",
title="Challenges with Sign Language Datasets",
bookTitle="Sign Language Machine Translation",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="117--139",
abstract="Sign Languages are the primary means of communication more than half a million people in Europe alone. However, the development of sign language recognition and translation tools is slowed down by a series of obstacles concerning resource scarcity and, when data is available, in standardisation issues in the available data.",
isbn="978-3-031-47362-3",
doi="10.1007/978-3-031-47362-3_5",
url="https://doi.org/10.1007/978-3-031-47362-3_5",
timestamp    = {2024-11-15}
}
@inproceedings{SingOn,
  abstract     = {{The SignON project (www.signon-project.eu) focuses on the research and development of a Sign Language (SL) translation mobile application and an open communications framework. SignON rectifies the lack of technology and services for the automatic translation between signed and spoken languages, through an inclusive, humancentric solution which facilitates communication between deaf, hard of hearing (DHH) and hearing individuals. We present an overview of the current status of the project, describing the milestones reached to date and the approaches that are being developed to address the challenges and peculiarities of Sign Language Machine Translation (SLMT).}},
  author       = {{Shterionov, Dimitar and De Sisto, Mirella and Vandeghinste, Vincent and Brady, Aoife and De Coster, Mathieu and Leeson, Lorraine and Blat, Josep and Picron, Frankie and Scipioni, Marcello and Parikh, Aditya and ten Bosch, Louis and O'Flaherty, John and Dambre, Joni and Rijckaert, Jorn}},
  booktitle    = {{Proceedings of the 23rd Annual Conference of the European Association for Machine Translation}},
  editor       = {{Macken, Lieve and Rufener, Andrew and Van den Bogaert, Joachim and Daems, Joke and Tezcan, Arda and Vanroy, Bram and Fonteyne, Margot and Barrault, Loïc and Costa-jussà, Marta R. and Kemp, Ellie and Pilos, Spyridon and Declercq, Christophe and Koponen, Maarit and Forcada, Mikel L. and Scarton, Carolina and Moniz, Helena}},
  isbn         = {{9789464597622}},
  language     = {{eng}},
  location     = {{Ghent, Belgium}},
  pages        = {{325--326}},
  publisher    = {{European Association for Machine Translation}},
  title        = {{Sign language translation : ongoing development, challenges and innovations in the SignON project}},
  url          = {https://aclanthology.org/2022.eamt-1.52},
  year         = {{2022}},
}

@inproceedings{shterionov-etal-2024-signon,
    title = "{S}ign{ON} {--} a Co-creative Machine Translation for Sign and Spoken Languages (end-of-project results, contributions and lessons learned)",
    author = "Shterionov, Dimitar  and
      Vandeghinste, Vincent  and
      Sisto, Mirella  and
      Brady, Aoife  and
      De Coster, Mathieu  and
      Leeson, Lorraine  and
      Way, Andy  and
      Blat, Josep  and
      Picron, Frankie  and
      Landuyt, Davy  and
      Scipioni, Marcello  and
      Parikh, Aditya  and
      Bosch, Louis  and
      O{'}Flaherty, John  and
      Dambre, Joni  and
      Brosens, Caro  and
      Rijckaert, Jorn  and
      Ubieto, V{\'\i}ctor  and
      Vanroy, Bram  and
      Gomez, Santiago  and
      Schuurman, Ineke  and
      Labaka, Gorka  and
      N{\'u}{\~n}ez-Marcos, Adri{\'a}n  and
      Murtagh, Irene  and
      McGill, Euan  and
      Saggion, Horacio",
    editor = "Scarton, Carolina  and
      Prescott, Charlotte  and
      Bayliss, Chris  and
      Oakley, Chris  and
      Wright, Joanna  and
      Wrigley, Stuart  and
      Song, Xingyi  and
      Gow-Smith, Edward  and
      Forcada, Mikel  and
      Moniz, Helena",
    booktitle = "Proceedings of the 25th Annual Conference of the European Association for Machine Translation (Volume 2)",
    month = jun,
    year = "2024",
    address = "Sheffield, UK",
    publisher = "European Association for Machine Translation (EAMT)",
    url = "https://aclanthology.org/2024.eamt-2.25",
    pages = "49--50",
    abstract = "SignON, a 3-year Horizon 20202 project addressing the lack of technology and services for MT between sign languages (SLs) and spoken languages (SpLs) ended in December 2023. SignON was unprecedented. Not only it addressed the wider complexity of the aforementioned problem {--} from research and development of recognition, translation and synthesis, through development of easy-to-use mobile applications and a cloud-based framework to do the {``}heavy lifting{''} as well as to establishing ethical, privacy and inclusivenesspolicies and operation guidelines {--} but also engaged with the deaf and hard of hearing communities in an effective co-creation approach where these main stakeholders drove the development in the right direction and had the final say.Currently we are witnessing advances in natural language processing for SLs, including MT. SignON was one of the largest projects that contributed to this surge with 17 partners and more than 60 consortium members, working in parallel with other international and European initiatives, such as project EASIER and others.",
		timestamp    = {2024-11-15}
}
