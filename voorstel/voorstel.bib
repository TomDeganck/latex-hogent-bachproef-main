@InBook{Cooper2011,
  author    = {Cooper, Helen and Holt, Brian and Bowden, Richard},
  editor    = {Moeslund, Thomas B. and Hilton, Adrian and Kr{\"u}ger, Volker and Sigal, Leonid},
  pages     = {539--562},
  publisher = {Springer London},
  title     = {Sign Language Recognition},
  year      = {2011},
  address   = {London},
  isbn      = {978-0-85729-997-0},
  abstract  = {This chapter covers the key aspects of sign-language recognition (SLR), starting with a brief introduction to the motivations and requirements, followed by a pr{\'e}cis of sign linguistics and their impact on the field. The types of data available and the relative merits are explored allowing examination of the features which can be extracted. Classifying the manual aspects of sign (similar to gestures) is then discussed from a tracking and non-tracking viewpoint before summarising some of the approaches to the non-manual aspects of sign languages. Methods for combining the sign classification results into full SLR are given showing the progression towards speech recognition techniques and the further adaptations required for the sign specific case. Finally the current frontiers are discussed and the recent research presented. This covers the task of continuous sign recognition, the work towards true signer independence, how to effectively combine the different modalities of sign, making use of the current linguistic research and adapting to larger more noisy data sets.},
  booktitle = {Visual Analysis of Humans: Looking at People},
  doi       = {10.1007/978-0-85729-997-0_27},
  timestamp = {2024-10-29},
  url       = {https://doi.org/10.1007/978-0-85729-997-0_27},
}

@article{10.56726/irjmets32712,
  title = {Real time sign language recognition using computer vision},
  journal = {International Research Journal of Modernization in Engineering Technology and Science},
  year = {2023},
  timestamp = {2024-11-04},
  doi = {10.56726/irjmets32712}
}

@article{10.1093/ijl/ecy008,
  author = {Vermeerbergen, M. and Herreweghe, M.},
  title = {Looking back while moving forward: the impact of societal and technological developments on flemish sign language lexicographic practices},
  journal = {International Journal of Lexicography},
  year = {2018},
  volume = {31},
  issue = {2},
  pages = {167-195},
  timestamp = {2024-11-04},
  doi = {10.1093/ijl/ecy008}
}

@article{10.1109/iccv.2019.00550,
  author = {Shi, B. and Rio, A. and Keane, J. and Brentari, D. and Shakhnarovich, G. and Livescu, K.},
  title = {Fingerspelling recognition in the wild with iterative visual attention},
  year = {2019},
  timestamp = {2024-11-04},
  doi = {10.1109/iccv.2019.00550}
}

@article{10.11591/csit.v3i3.p186-193,
  author = {Ahmed, S. and Shafiq, H. and Raheel, Y. and Chishti, N. and Asad, S.},
  title = {Pakistan sign language to urdu translator using kinect},
  journal = {Computer Science and Information Technologies},
  year = {2022},
  volume = {3},
  issue = {3},
  pages = {186-193},
  timestamp = {2024-11-04},
  doi = {10.11591/csit.v3i3.p186-193}
}

@article{10.48550/arxiv.2012.15840,
  author = {Zheng, S. and Lu, J. and Zhao, H. and Zhu, X. and Luo, Z. and Wang, Y. and Fu, Y. and Feng, J. and Xiang, T. and Torr, P. and Zhang, L.},
  title = {Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers},
  year = {2020},
  timestamp = {2024-11-04},
  doi = {10.48550/arxiv.2012.15840}
}

@article{10.57197/jdr-2023-0047,
  author = {Marzouk, R.},
  title = {Sign language recognition using artificial rabbits optimizer with siamese neural network for persons with disabilities},
  year = {2023},
  volume = {2},
  issue = {4},
  timestamp = {2024-11-04},
  doi = {10.57197/jdr-2023-0047}
}

@article{10.22214/ijraset.2023.52628,
  author = {Gade, P.},
  title = {Ai-based sign language recognition system for deaf and hard-of-hearing individuals},
  journal = {International Journal for Research in Applied Science and Engineering Technology},
  year = {2023},
  volume = {11},
  issue = {5},
  pages = {4472-4476},
  timestamp = {2024-11-04},
doi = {10.22214/ijraset.2023.52628}
}

@article{10.1109/wacv45572.2020.9093512,
  author = {Li, D. and Opazo, C. and Yu, X. and Li, H.},
  title = {Word-level deep sign language recognition from video: a new large-scale dataset and methods comparison},
  year = {2020},
  pages = {1448-1458},
  timestamp = {2024-11-04},
  doi = {10.1109/wacv45572.2020.9093512}
}

@article{10.18178/ijmlc.2019.9.6.879,
  author = {Tolentino, L. and Juan, R. and Thio-ac, A. and Pamahoy, M. and Forteza, J. and Garcia, X.},
  title = {Static sign language recognition using deep learning},
  journal = {International Journal of Machine Learning and Computing},
  year = {2019},
  volume = {9},
  issue = {6},
  pages = {821-827},
  timestamp = {2024-11-04},
  doi = {10.18178/ijmlc.2019.9.6.879}
}

@article{10.24003/emitter.v9i1.613,
  author = {H, M.},
  title = {Indian sign language recognition through hybrid convnet-lstm networks},
  journal = {Emitter International Journal of Engineering Technology},
  year = {2021},
  volume = {9},
  issue = {1},
  pages = {182-203},
  timestamp = {2024-11-04},
  doi = {10.24003/emitter.v9i1.613}
}

@article{10.18653/v1/2020.conll-1.51,
  author = {Imashev, A. and Mukushev, M. and Kimmelman, V. and Sandygulova, A.},
  title = {A dataset for linguistic understanding, visual evaluation, and recognition of sign languages: the k-rsl},
  year = {2020},
  timestamp = {2024-11-04},
  doi = {10.18653/v1/2020.conll-1.51}
}

@article{10.1049/iet-cvi.2017.0598,
  author = {Elpeltagy, M. and Abdelwahab, M. and Hussein, M. and Shoukry, A. and Shoala, A. and Galal, M.},
  title = {Multi‐modality‐based arabic sign language recognition},
  journal = {Iet Computer Vision},
  year = {2018},
  volume = {12},
  issue = {7},
  pages = {1031-1039},
  timestamp = {2024-11-04},
doi = {10.1049/iet-cvi.2017.0598}
}

@article{10.1109/ictas.2019.8703631,
  author = {Henney, A. and Tucker, W.},
  title = {Video relay service for deaf people using webrtc},
  year = {2019},
  pages = {1-6},
  timestamp = {2024-11-04},
doi = {10.1109/ictas.2019.8703631}
}

@article{10.22214/ijraset.2022.46470,
  author = {Gattani, P. and Laddha, S. and Srivastava, S. and BM, V.},
  title = {A survey on real time sign language detector using ml},
  journal = {International Journal for Research in Applied Science and Engineering Technology},
  year = {2022},
  volume = {10},
  issue = {8},
  pages = {1635-1643},
  timestamp = {2024-11-04},
doi = {10.22214/ijraset.2022.46470}
}

@article{10.1109/compsac51774.2021.00173,
  author = {Buckley, N. and Sherrett, L. and Secco, E.},
  title = {A cnn sign language recognition system with single &amp; double-handed gestures},
  year = {2021},
  timestamp = {2024-11-04},
doi = {10.1109/compsac51774.2021.00173}
}

@article{10.1109/access.2020.2990699,
  author = {Aly, S. and Aly, W.},
  title = {Deeparslr: a novel signer-independent deep learning framework for isolated arabic sign language gestures recognition},
  journal = {Ieee Access},
  year = {2020},
  volume = {8},
  pages = {83199-83212},
  timestamp = {2024-11-04},
doi = {10.1109/access.2020.2990699}
}

@article{10.1109/access.2020.2993650,
  author = {Papastratis, I. and Dimitropoulos, K. and Konstantinidis, D. and Daras, P.},
  title = {Continuous sign language recognition through cross-modal alignment of video and text embeddings in a joint-latent space},
  journal = {Ieee Access},
  year = {2020},
  volume = {8},
  pages = {91170-91180},
  timestamp = {2024-11-04},
doi = {10.1109/access.2020.2993650}
}

@article{10.13053/rcs-148-3-17,
  author = {Martínez-Gutiérrez, M. and Rojano-Cáceres, J. and Benítez–Guerrero, E. and Sánchez-Barrera, H.},
  title = {Data acquisition software for sign language recognition},
  journal = {Research in Computing Science},
  year = {2019},
  volume = {148},
  issue = {3},
  pages = {205-211},
  timestamp = {2024-11-04},
  doi = {10.13053/rcs-148-3-17}
}

@article{10.1002/cpe.7653,
  author = {Robert, E. and Hemanth, D.},
  title = {A review on computational methods based automated sign language recognition system for hearing and speech impaired community},
  journal = {Concurrency and Computation Practice and Experience},
  year = {2023},
  volume = {35},
  issue = {9},
  timestamp = {2024-11-04},
  doi = {10.1002/cpe.7653}
}

@article{10.13053/rcs-148-6-19,
  author = {Rodríguez-Tapia, B. and Ochoa-Zezzatti, A. and Marrufo, Á. and Arballo, N. and Carlos, P.},
  title = {Sign language recognition based on emg signals through a hibrid intelligent system},
  journal = {Research in Computing Science},
  year = {2019},
  volume = {148},
  issue = {6},
  pages = {253-262},
  timestamp = {2024-11-04},
  doi = {10.13053/rcs-148-6-19}
}

@article{10.1007/978-3-030-66096-3_13,
  author = {Camgöz, N. and Varol, G. and Albanie, S. and Fox, N. and Bowden, R. and Zisserman, A. and Cormier, K.},
  title = {Slrtp 2020: the sign language recognition, translation &amp; production workshop},
  year = {2020},
  pages = {179-185},
  timestamp = {2024-11-04},
  doi = {10.1007/978-3-030-66096-3_13}
}

@article{10.32604/cmc.2022.025953,
  author = {Hussain, M. and Shaoor, A. and Alsuhibany, S. and Ghadi, Y. and Shloul, T. and Jalal, A. and Park, J.},
  title = {Intelligent sign language recognition system for e-learning context},
  journal = {Computers Materials & Continua},
  year = {2022},
  volume = {72},
  issue = {3},
  pages = {5327-5343},
  timestamp = {2024-11-04},
  doi = {10.32604/cmc.2022.025953}
}

@article{10.48550/arxiv.2010.11929,
  author = {Dosovitskiy, A.},
  title = {An image is worth 16x16 words: transformers for image recognition at scale},
  year = {2020},
  timestamp = {2024-11-04},
  doi = {10.48550/arxiv.2010.11929}
}

@article{10.1145/3308561.3353774,
  author = {Bragg, D. and Koller, O. and Bellard, M. and Berke, L. and Boudreault, P. and Braffort, A. and Caselli, N. and Huenerfauth, M. and Kacorri, H. and Verhoef, T. and Vogler, C. and Morris, M.},
  title = {Sign language recognition, generation, and translation},
  year = {2019},
  pages = {16-31},
  timestamp = {2024-11-04},
  doi = {10.1145/3308561.3353774}
}

@Article{vanmeerbergen2000simultane,
  author    = {Vanmeerbergen, Myriam},
  journal   = {Handelingen-Koninklijke Zuid-Nederlandse maatschappij voor taal-en letterkunde en geschiedenis},
  title     = {Simultane constructies in de Vlaamse Gebarentaal},
  year      = {2000},
  volume    = {54},
  publisher = {Koninklijke Zuid-Nederlandse Maatschappij voor Taal-en Letterkunde en~…},
  timestamp = {2024-11-05},
  url={https://openjournals.ugent.be/kzm/article/id/72219/download/pdf/}
}

@phdthesis{469340,
  abstract     = {De doelstelling van dit proefschrift is een eerste uitgebreide beschrijving te geven van de fonologische vormelementen en structuren in de Vlaamse Gebarentaal (VGT) en na te gaan wat daarbij de plaats van iconiciteit binnen (of buiten?) een fonologisch model kan zijn. De focus ligt op het manuele gedeelte van gebaren, d.w.z. de vier parameters handvorm, oriëntatie van de hand, articulatieplaats en beweging van het gebaar. De basis voor de fonologische beschrijving is een gedetailleerde fonetische transcriptie van een 2400-tal geïsoleerde VGT-gebaren (gebaren in citeervorm), getranscribeerd met een vereenvoudigde versie van SignPhon (Crasborn et al. 1997). De fonologische analyse van die gebaren gebeurt aan de hand van een model dat ontwikkeld is voor de Nederlandse Gebarentaal (NGT), met name het dependentiemodel van Van der Kooij (2002).},
  author       = {Demey, Eline},
  keywords     = {transcriptie,fonologie,iconiciteit,dependentiemodel,Vlaamse Gebarentaal (VGT)},
  language     = {dut},
  pages        = {550},
  publisher    = {Universiteit Gent. Faculteit Letteren en Wijsbegeerte},
  school       = {Ghent University},
  title        = {Fonologie van de Vlaamse Gebarentaal: distinctiviteit en iconiciteit},
  url          = {http://lib.ugent.be/fulltxt/RUG01/000/938/845/RUG01-000938845_2010_0001_AC.pdf},
  year         = {2005},
  timestamp = {2024-11-05}
}
@article{10.52756/ijerr.2023.v34spl.004,
  author = {Yavanamandha, P. and Keerthana, B. and Jahnavi, P. and Rao, K. V. and Kumar, C. R.},
  title = {Machine learning-based gesture recognition for communication with the deaf and dumb},
  journal = {International Journal of Experimental Research and Review},
  year = {2023},
  volume = {34},
  issue = {Special Vo},
  pages = {26-35},
  doi = {10.52756/ijerr.2023.v34spl.004},
  timestamp = {2024-11-15}
}
@article{10.17485/ijst/v16i45.2583,
  author = {Duraisamy, P. and Abinayasrijanani, A. and Candida, M. A. and Babu, P. D.},
  title = {Transforming sign language into text and speech through deep learning technologies},
  journal = {Indian Journal of Science and Technology},
  year = {2023},
  volume = {16},
  issue = {45},
  pages = {4177-4185},
  doi = {10.17485/ijst/v16i45.2583},
  timestamp = {2024-11-15}
}
@misc{Corpus_VGT,
  abstract     = {Het Corpus VGT is een verzameling van video’s in Vlaamse Gebarentaal. We verzamelden in totaal meer dan 5 TB of 140 uren video. 120 dove mensen werkten mee aan het Corpus VGT als informanten. Bij het selecteren van de informanten werd rekening gehouden met leeftijd, regio en gender. Per twee gaven we hen een reeks thema’s om over te praten: vertellen van een verhaal, maken van afspraken, discussiëren over een thema, vertellen over hun schooltijd enz. Deze conversaties namen we op video op en we monteerden ze per opdracht. De gesprekken in Vlaamse Gebarentaal annoteerden we vervolgens door ze te glossen.

Het Corpus VGT is gemaakt om onderzoek naar Vlaamse Gebarentaal te doen. Omdat er in andere landen ook al gebarentaalcorpora zijn samengesteld, kan men ook vergelijkend onderzoek tussen Vlaamse Gebarentaal en andere gebarentalen doen. Daarnaast kunnen docenten het corpus gebruiken in hun lessen Vlaamse Gebarentaal. Het Corpus VGT is ook een middel om Vlaamse Gebarentaal en aspecten van de Vlaamse Dovencultuur te documenteren en te bewaren.},
  author       = {Van Herreweghe, Mieke and Vermeerbergen, Myriam and Demey, Eline and De Durpel, Hannes and Nyffels, Hilde and Verstraete, Sam},
  keywords     = {Vlaamse Gebarentaal,Corpuslinguïstiek},
  language     = {dut},
  title        = {Het Corpus VGT. Een digitaal open access corpus van videos and annotaties van Vlaamse Gebarentaal, ontwikkeld aan de Universiteit Gent ism KU Leuven. <www.corpusvgt.be>},
  url          = {http://www.corpusvgt.ugent.be/},
  year         = {2015},
  timestamp = {2024-11-15}
}
@misc{VGT_Signbank, url={https://signbank.vlaamsegebarentaal.be/accounts/login/?next=/datasets/available},
timestamp = {2024-11-15} }
@misc{Over_het_Corpus_VGT, url={https://www.corpusvgt.be/},
timestamp = {2024-11-15} }
@INPROCEEDINGS{9491631,
  author={Albashish, Dheeb and Al-Sayyed, Rizik and Abdullah, Azizi and Ryalat, Mohammad Hashem and Ahmad Almansour, Nedaa},
  booktitle={2021 International Conference on Information Technology (ICIT)}, 
  title={Deep CNN Model based on VGG16 for Breast Cancer Classification}, 
  year={2021},
  volume={},
  number={},
  pages={805-810},
  keywords={Deep learning;Adaptation models;Visualization;Histopathology;Computational modeling;Benchmark testing;Feature extraction;Deep learning;VGG16;BreaKHis histopathology dataset;feature extraction;pre-trained model},
  doi={10.1109/ICIT52682.2021.9491631},
  timestamp = {2024-11-15}
}
@incollection{torrey2010transfer,
  title={Transfer learning},
  author={Torrey, Lisa and Shavlik, Jude},
  booktitle={Handbook of research on machine learning applications and trends: algorithms, methods, and techniques},
  pages={242--264},
  year={2010},
  publisher={IGI global},
  timestamp = {2024-11-15}
}


@article{Abu-Jamie2022-ABUCOS-2,
	author = {Tanseem N. Abu{-}Jamie and Samy S. Abu{-}Naser},
	journal = {International Journal of Academic Engineering Research (IJAER)},
	number = {6},
	pages = {36--46},
	title = {Classification of Sign-Language Using Vgg16},
	volume = {6},
	year = {2022},
  timestamp = {2024-11-15}
}
@article{Wang2022,
  author = {Shiqi Wang and Kankan Wang and Tingping Yang and Yiming Li and Di Fan},
  title = {Improved 3D-ResNet sign language recognition algorithm with enhanced hand features},
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {17812},
  year = {2022},
  doi = {10.1038/s41598-022-21636-z},
  url = {https://doi.org/10.1038/s41598-022-21636-z},
  abstract = {In sign language video, the hand region is small, the resolution is low, the motion speed is fast, and there are cross occlusion and blur phenomena, which have a great impact on sign language recognition rate and speed, and are important factors restricting sign language recognition performance. To solve these problems, this paper proposes an improved 3D-ResNet sign language recognition algorithm with enhanced hand features, aiming to highlight the features of both hands, solve the problem of missing more effective information when relying only on global features, and improve the accuracy of sign language recognition. The proposed method has two improvements. Firstly, the algorithm detects the left and right hand regions based on the improved EfficientDet network, uses the improved Bi-FPN module and dual channel and spatial attention module are used to enhance the detection ability of the network for small targets like hand. Secondly, the improved residual module is used to improve the 3D-ResNet18 network to extract sign language features. The global, the left-hand and the right-hand image sequences are divided into three branches for feature extraction and fusion, so as to strengthen the attention to hand features, strengthen the representation ability of sign language features, and achieve the purpose of improving the accuracy of sign language recognition. In order to verify the performance of this algorithm, a series of experiments are carried out on CSL dataset. For example, in the experiments of hand detection algorithm and sign language recognition algorithm, the performance indicators such as Top-N, mAP, FLOPs and Parm are applied to find the optimal algorithm framework. The experimental results show that the Top1 recognition accuracy of this algorithm reaches 91.12%, which is more than 10% higher than that of C3D, P3D and 3D-ResNet basic networks. From the performance indicators of Top-N, mAP, FLOPs, Parm and so on, the performance of the algorithm in this paper is better than several algorithms in recent three years, such as I3D+BLSTM, B3D ResNet, AM-ResC3D+RCNN and so on. The results show that the hand detection network with enhanced hand features and three-dimensional convolutional neural network proposed in this paper can achieve higher accuracy of sign language recognition.},
  issn = {2045-2322},
  note = {Published: 2022/10/24},
  timestamp = {2024-11-15}
}
@Unknown{Coster2023,
  author = {Coster, Mathieu and Rushe, Ellen and Holmes, Ruth and Ventresque, Anthony and Dambre, J.},
  doi    = {10.48550/arXiv.2306.17558},
  month  = {06},
  title  = {Towards the extraction of robust sign embeddings for low resource sign language recognition},
  year   = {2023},
  timestamp = {2024-11-15}
}
